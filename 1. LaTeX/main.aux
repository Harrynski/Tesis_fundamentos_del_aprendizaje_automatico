\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Un acercamiento amistoso a la Teoría del Aprendizaje Estadístico}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introducción}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}El aprendizaje}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}La historia del aprendizaje automático}{4}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Conceptos básicos del aprendizaje}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Pérdida y Riesgo}{7}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Generalización}{9}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Consistencia}{9}{subsection.1.4.3}\protected@file@percent }
\newlabel{def:consistencias}{{1.4.3.1}{11}{}{Item.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Sobreajuste y subajuste}{12}{subsection.1.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Dos posibles modelos de regresión, uno lineal y otro no, para un conjunto de datos dado.}}{12}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Sobreajuste y subajuste}{{1.1}{12}{Dos posibles modelos de regresión, uno lineal y otro no, para un conjunto de datos dado}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Los dilemas sesgo-varianza y estimación-aproximación}{13}{subsection.1.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Error de estimación y de aproximación.}}{14}{figure.caption.3}\protected@file@percent }
\newlabel{fig:error de estimación y aproximación}{{1.2}{14}{Error de estimación y de aproximación}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Compromiso entre error de estimación y de aproximación. Si el espacio funcional $\mathcal  {F}$ que define el algoritmo de aprendizaje es pequeño, es decir es poco complejo, el error de estimación es bajo, pero el de aproximación es alto, y estamos en una situación proclive al subajuste. Si, por otro lado, $\mathcal  {F}$ es complejo, el error de estimación es alto y el de aproximación bajo, y tendemos al sobreajuste. El error ideal es usualmente obtenido con una complejidad moderada.}}{15}{figure.caption.4}\protected@file@percent }
\newlabel{fig:Compromiso entre error de estimación y de aproximación.}{{1.3}{15}{Compromiso entre error de estimación y de aproximación. Si el espacio funcional $\mathcal {F}$ que define el algoritmo de aprendizaje es pequeño, es decir es poco complejo, el error de estimación es bajo, pero el de aproximación es alto, y estamos en una situación proclive al subajuste. Si, por otro lado, $\mathcal {F}$ es complejo, el error de estimación es alto y el de aproximación bajo, y tendemos al sobreajuste. El error ideal es usualmente obtenido con una complejidad moderada}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}El clasificador de los $k$ vecinos más cercanos}{15}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Minimización del riesgo empírico}{19}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}La ley de los grandes números}{20}{subsection.1.6.1}\protected@file@percent }
\newlabel{eq:Chernoff}{{1.2}{21}{La ley de los grandes números}{equation.1.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Inconsistencia en la minimización del riesgo empírico}{22}{subsection.1.6.2}\protected@file@percent }
\newlabel{Inconsistencia en la minimización del riesgo empírico}{{1.6.2}{22}{Inconsistencia en la minimización del riesgo empírico}{subsection.1.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Convergencia uniforme}{23}{subsection.1.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Representación simplificada de la convergencia del riesgo empírico al riesgo verdadero. El eje \(x\) representa una dimensión de la clase de funciones \(F\), mientras que el eje \(y\) denota el riesgo. Para cada función fija \(f\), la ley de los grandes números nos dice que, a medida que el tamaño de la muestra tiende a infinito, el riesgo empírico \(\mathcal  {R}_{\text  {emp}}(f)\) converge al riesgo verdadero \(\mathcal  {R}(f)\) (indicado por la flecha). Sin embargo, esto no implica que, en el límite de tamaños de muestra infinitos, el minimizador del riesgo empírico, \(f_n\), lleve a un valor del riesgo tan bueno como el del mejor \(f_F\) en la clase de funciones. Para que esto sea cierto, requerimos la convergencia uniforme de \(\mathcal  {R}_{\text  {emp}}(f)\) a \(\mathcal  {R}(f)\) sobre todas las funciones en \(F\). (Adaptado de Schölkopf y Smola, 2002).}}{24}{figure.caption.5}\protected@file@percent }
\newlabel{riesgo vs riesgo empirico}{{1.4}{24}{Representación simplificada de la convergencia del riesgo empírico al riesgo verdadero. El eje \(x\) representa una dimensión de la clase de funciones \(F\), mientras que el eje \(y\) denota el riesgo. Para cada función fija \(f\), la ley de los grandes números nos dice que, a medida que el tamaño de la muestra tiende a infinito, el riesgo empírico \(\mathcal {R}_{\text {emp}}(f)\) converge al riesgo verdadero \(\mathcal {R}(f)\) (indicado por la flecha). Sin embargo, esto no implica que, en el límite de tamaños de muestra infinitos, el minimizador del riesgo empírico, \(f_n\), lleve a un valor del riesgo tan bueno como el del mejor \(f_F\) en la clase de funciones. Para que esto sea cierto, requerimos la convergencia uniforme de \(\mathcal {R}_{\text {emp}}(f)\) a \(\mathcal {R}(f)\) sobre todas las funciones en \(F\). (Adaptado de Schölkopf y Smola, 2002)}{figure.caption.5}{}}
\newlabel{eq:cota_uniforme}{{1.3}{24}{Convergencia uniforme}{equation.1.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Cotas de generalización y medidas de capacidad}{26}{section.1.7}\protected@file@percent }
\newlabel{eq:med_unif_riesgo_riesgo_emp}{{1.4}{26}{Cotas de generalización y medidas de capacidad}{equation.1.7.4}{}}
\newlabel{eq: chernoff acotada por m}{{1.5}{27}{Cotas de generalización y medidas de capacidad}{equation.1.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Simetrización}{27}{subsection.1.7.1}\protected@file@percent }
\newlabel{lema de simetrizacion}{{1.7.1.1}{28}{de simetrización}{lem.1.7.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}El coeficiente de fragmentación}{29}{subsection.1.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Fragmentación de muestras en $\mathbb  {R}^2$ de tres y cuatro puntos.}}{30}{figure.caption.6}\protected@file@percent }
\newlabel{fragmentacion perceptron}{{1.5}{30}{Fragmentación de muestras en $\mathbb {R}^2$ de tres y cuatro puntos}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.3}Cotas de convergencia uniforme}{30}{subsection.1.7.3}\protected@file@percent }
\newlabel{eq: cota convergencia coeficiente fragmentacion}{{1.6}{31}{Cotas de convergencia uniforme}{equation.1.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.4}Cotas de generalización}{32}{subsection.1.7.4}\protected@file@percent }
\newlabel{eq: cota generalizacion}{{1.7}{32}{Cotas de generalización}{equation.1.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.5}La dimensión VC}{33}{subsection.1.7.5}\protected@file@percent }
\newlabel{def:cota combinatoria de dimensión VC}{{1.7.5.1}{34}{}{lem.1.7.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.6}Complejidad de Rademacher}{35}{subsection.1.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.7}Cotas con grandes márgenes de separación}{36}{subsection.1.7.7}\protected@file@percent }
\newlabel{def:margen}{{1.7.7.1}{37}{}{dfn.1.7.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Margen de un clasificador lineal: las cruces representan puntos de entrenamiento con etiqueta \(+1\), mientras que los círculos representan puntos de entrenamiento con etiqueta \(-1\). La línea recta es el clasificador lineal \(f_n\), y la línea discontinua muestra el margen. La anchura \(\rho \) del margen está representada por la flecha.}}{37}{figure.caption.7}\protected@file@percent }
\newlabel{fig:Clasificación con margen}{{1.6}{37}{Margen de un clasificador lineal: las cruces representan puntos de entrenamiento con etiqueta \(+1\), mientras que los círculos representan puntos de entrenamiento con etiqueta \(-1\). La línea recta es el clasificador lineal \(f_n\), y la línea discontinua muestra el margen. La anchura \(\rho \) del margen está representada por la flecha}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.8}Conclusiones acerca de las cotas de generalización}{38}{subsection.1.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}La Teoría VC}{39}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.1}Definiciones preliminares}{39}{subsection.2.0.1}\protected@file@percent }
\newlabel{def: probabilidad en espacio muestral}{{2.1}{39}{Definiciones preliminares}{equation.2.0.1}{}}
\newlabel{def: funcion pi}{{2.2}{40}{Definiciones preliminares}{equation.2.0.2}{}}
\newlabel{def: combinatoria función crecimiento lineal}{{2.3}{42}{}{equation.2.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.2}La función de crecimiento}{43}{subsection.2.0.2}\protected@file@percent }
\newlabel{teo: cota polinomica de la función de crecimiento}{{2.0.2.1}{43}{}{thm.2.0.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.3}El Lema de simetrización}{44}{subsection.2.0.3}\protected@file@percent }
\newlabel{def Z2l prod interno}{{2.4}{46}{El Lema de simetrización}{equation.2.0.4}{}}
\newlabel{desigualdad P(C)}{{2.6}{46}{El Lema de simetrización}{equation.2.0.6}{}}
\newlabel{eq: theta como probabilidad}{{2.7}{47}{El Lema de simetrización}{equation.2.0.7}{}}
\newlabel{Chebyshev aplicada a frecuencia relativa}{{2.8}{47}{El Lema de simetrización}{equation.2.0.8}{}}
\newlabel{eq:probabilidad integral con permutaciones}{{2.9}{50}{El Lema de simetrización}{equation.2.0.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.4}Condiciones de convergencia uniforme casi segura}{52}{subsection.2.0.4}\protected@file@percent }
\newlabel{condicion nec y suf}{{2.10}{53}{}{equation.2.0.10}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Otros conceptos del Aprendizaje}{55}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Consistencia de Bayes y error de aproximación}{55}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Espacios funcionales anidados}{56}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Regularización}{57}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Los teoremas de la chancha y los veinte}{59}{section.3.2}\protected@file@percent }
\newlabel{eq:cota_parametro_b_distribuciones}{{3.1}{63}{Los teoremas de la chancha y los veinte}{equation.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Tomando en consideración todas las distribuciones posibles -eje X-, el clasificador 1 es en general mejor que el clasificador 2, y este último es muchísimo mejor para un subconjunto de distribuciones. Podemos pensar que 1 es un clasificador general y 2 está construído para un tipo de problemas específico. El teorema de la chancha y los veinte nos dice que el área debajo de ambas curvas debe ser igual, es decir que en promedio ambos clasificadores tienen el mismo error.}}{65}{figure.caption.8}\protected@file@percent }
\newlabel{fig:no_free_lunch_theorem}{{3.1}{65}{Tomando en consideración todas las distribuciones posibles -eje X-, el clasificador 1 es en general mejor que el clasificador 2, y este último es muchísimo mejor para un subconjunto de distribuciones. Podemos pensar que 1 es un clasificador general y 2 está construído para un tipo de problemas específico. El teorema de la chancha y los veinte nos dice que el área debajo de ambas curvas debe ser igual, es decir que en promedio ambos clasificadores tienen el mismo error}{figure.caption.8}{}}
\gdef \@abspage@last{67}
