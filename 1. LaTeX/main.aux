\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}El aprendizaje}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}La historia del aprendizaje automático}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Hacia la formalización}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pérdida y Riesgo}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Generalización}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Consistencia}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Sobreajuste y subajuste}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Dos posibles modelos de regresión, uno lineal y otro no, para un conjunto de datos dado.}}{10}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Sobreajuste y subajuste}{{1}{10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Los dilemas sesgo-varianza y estimación-aproximación}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Error de estimación y de aproximación.}}{12}{}\protected@file@percent }
\newlabel{fig:error de estimación y aproximación}{{2}{12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Compromiso entre error de estimación y de aproximación. Si el espacio funcional $\mathcal  {F}$ que define el algoritmo de aprendizaje es pequeño, es decir es poco complejo, el error de estimación es bajo, pero el de aproximación es alto, y estamos en una situación proclive al subajuste. Si, por otro lado, $\mathcal  {F}$ es complejo, el error de estimación es alto y el de aproximación bajo, y tendemos al sobreajuste. El error ideal es usualmente obtenido con una complejidad moderada.}}{13}{}\protected@file@percent }
\newlabel{fig:Compromiso entre error de estimación y de aproximación.}{{3}{13}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}El clasificador de los $k$ vecinos más cercanos}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Minimización del riesgo empírico}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}La ley de los grandes números}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Inconsistencia en la minimización del riesgo empírico}{19}{}\protected@file@percent }
\newlabel{Inconsistencia en la minimización del riesgo empírico}{{6.2}{19}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representación simplificada de la convergencia del riesgo empírico al riesgo verdadero. El eje \(x\) representa una dimensión de la clase de funciones \(F\), mientras que el eje \(y\) denota el riesgo. Para cada función fija \(f\), la ley de los grandes números nos dice que, a medida que el tamaño de la muestra tiende a infinito, el riesgo empírico \(R_{\text  {emp}}(f)\) converge al riesgo verdadero \(R(f)\) (indicado por la flecha). Sin embargo, esto no implica que, en el límite de tamaños de muestra infinitos, el minimizador del riesgo empírico, \(f_n\), lleve a un valor del riesgo tan bueno como el del mejor \(f_F\) en la clase de funciones. Para que esto sea cierto, requerimos la convergencia uniforme de \(R_{\text  {emp}}(f)\) a \(R(f)\) sobre todas las funciones en \(F\). (Adaptado de Schölkopf y Smola, 2002).}}{21}{}\protected@file@percent }
\newlabel{riesgo vs riesgo empirico}{{4}{21}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Bibliografía}{24}{}\protected@file@percent }
\gdef \@abspage@last{24}
