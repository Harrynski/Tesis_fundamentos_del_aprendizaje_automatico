\babel@toc {spanish}{}\relax 
\contentsline {chapter}{\numberline {1}Un acercamiento amistoso a la Teoría del Aprendizaje Estadístico}{3}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introducción}{3}{section.1.1}%
\contentsline {section}{\numberline {1.2}La historia del aprendizaje automático}{4}{section.1.2}%
\contentsline {section}{\numberline {1.3}Conceptos básicos del aprendizaje}{6}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Pérdida y Riesgo}{7}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Espacios de funciones y el clasificador de Bayes}{8}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Generalización}{9}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Algortimos de aprendizaje}{10}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Consistencia}{11}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Dilemas del aprendizaje}{13}{subsection.1.3.6}%
\contentsline {section}{\numberline {1.4}El clasificador de los $k$ vecinos más cercanos}{18}{section.1.4}%
\contentsline {section}{\numberline {1.5}Minimización del riesgo empírico}{22}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Aplicando la ley de los grandes números}{22}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Inconsistencia en la minimización del riesgo empírico}{24}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Convergencia uniforme}{25}{subsection.1.5.3}%
\contentsline {section}{\numberline {1.6}Cotas de generalización y medidas de capacidad}{29}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Simetrización}{30}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}El coeficiente de fragmentación}{31}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3}Cotas de convergencia uniforme}{33}{subsection.1.6.3}%
\contentsline {subsection}{\numberline {1.6.4}Cotas de generalización}{35}{subsection.1.6.4}%
\contentsline {subsection}{\numberline {1.6.5}La dimensión VC}{36}{subsection.1.6.5}%
\contentsline {subsection}{\numberline {1.6.6}Complejidad de Rademacher}{38}{subsection.1.6.6}%
\contentsline {subsection}{\numberline {1.6.7}Cotas con grandes márgenes de separación}{39}{subsection.1.6.7}%
\contentsline {subsection}{\numberline {1.6.8}Conclusiones acerca de las cotas de generalización}{41}{subsection.1.6.8}%
\contentsline {chapter}{\numberline {2}La Teoría VC}{42}{chapter.2}%
\contentsline {subsection}{\numberline {2.0.1}Definiciones preliminares}{42}{subsection.2.0.1}%
\contentsline {subsection}{\numberline {2.0.2}La función de crecimiento}{46}{subsection.2.0.2}%
\contentsline {subsection}{\numberline {2.0.3}El Lema de simetrización}{47}{subsection.2.0.3}%
\contentsline {subsection}{\numberline {2.0.4}Condiciones de convergencia uniforme casi segura}{55}{subsection.2.0.4}%
\contentsline {chapter}{\numberline {3}Otros conceptos del Aprendizaje}{67}{chapter.3}%
\contentsline {section}{\numberline {3.1}Consistencia de Bayes y error de aproximación}{67}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Espacios funcionales anidados}{68}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Regularización}{69}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Los teoremas de la chancha y los veinte}{71}{section.3.2}%
