\babel@toc {spanish}{}\relax 
\contentsline {chapter}{\numberline {1}Un acercamiento amistoso a la Teoría del Aprendizaje Estadístico}{3}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introducción}{3}{section.1.1}%
\contentsline {section}{\numberline {1.2}La historia del aprendizaje automático}{4}{section.1.2}%
\contentsline {section}{\numberline {1.3}Conceptos básicos del aprendizaje}{6}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Pérdida y Riesgo}{7}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Espacios de funciones y el clasificador de Bayes}{8}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Generalización}{9}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Algortimos de aprendizaje}{10}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Consistencia}{11}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Sobreajuste y subajuste}{13}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Los dilemas sesgo-varianza y estimación-aproximación}{15}{subsection.1.3.7}%
\contentsline {section}{\numberline {1.4}El clasificador de los $k$ vecinos más cercanos}{16}{section.1.4}%
\contentsline {section}{\numberline {1.5}Minimización del riesgo empírico}{21}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Aplicando la ley de los grandes números}{21}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Inconsistencia en la minimización del riesgo empírico}{23}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Convergencia uniforme}{24}{subsection.1.5.3}%
\contentsline {section}{\numberline {1.6}Cotas de generalización y medidas de capacidad}{28}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Simetrización}{29}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}El coeficiente de fragmentación}{30}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3}Cotas de convergencia uniforme}{32}{subsection.1.6.3}%
\contentsline {subsection}{\numberline {1.6.4}Cotas de generalización}{34}{subsection.1.6.4}%
\contentsline {subsection}{\numberline {1.6.5}La dimensión VC}{35}{subsection.1.6.5}%
\contentsline {subsection}{\numberline {1.6.6}Complejidad de Rademacher}{37}{subsection.1.6.6}%
\contentsline {subsection}{\numberline {1.6.7}Cotas con grandes márgenes de separación}{38}{subsection.1.6.7}%
\contentsline {subsection}{\numberline {1.6.8}Conclusiones acerca de las cotas de generalización}{40}{subsection.1.6.8}%
\contentsline {chapter}{\numberline {2}La Teoría VC}{41}{chapter.2}%
\contentsline {subsection}{\numberline {2.0.1}Definiciones preliminares}{41}{subsection.2.0.1}%
\contentsline {subsection}{\numberline {2.0.2}La función de crecimiento}{45}{subsection.2.0.2}%
\contentsline {subsection}{\numberline {2.0.3}El Lema de simetrización}{46}{subsection.2.0.3}%
\contentsline {subsection}{\numberline {2.0.4}Condiciones de convergencia uniforme casi segura}{54}{subsection.2.0.4}%
\contentsline {chapter}{\numberline {3}Otros conceptos del Aprendizaje}{66}{chapter.3}%
\contentsline {section}{\numberline {3.1}Consistencia de Bayes y error de aproximación}{66}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Espacios funcionales anidados}{67}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Regularización}{68}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Los teoremas de la chancha y los veinte}{70}{section.3.2}%
