\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Resultados Preliminares}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Medida}{7}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Espacios de Probabilidad}{9}{section.1.2}\protected@file@percent }
\newlabel{primer lema de Borel-Cantelli}{{1.2.2}{10}{Primer Lema de Borel-Cantelli}{lem.1.2.2}{}}
\newlabel{segundo lema de Borel-Cantelli}{{1.2.3}{10}{Segundo Lema de Borel-Cantelli}{lem.1.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Variables Aleatorias}{11}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Variables Aleatorias Discretas}{13}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Variables Aleatorias Continuas}{14}{subsection.1.3.2}\protected@file@percent }
\newlabel{def distribucion normal}{{1.3.4}{15}{Variable Aleatoria Normal}{ej.1.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Transformaciones de Variables Aleatorias}{15}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Vectores Aleatorios}{16}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Vectores Aleatorios Discretos}{17}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Vectores Aleatorios Continuos}{17}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Momentos de una variable aleatoria}{18}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Esperanza}{18}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}La Varianza y otros momentos}{20}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Tipos de convergencia}{22}{section.1.6}\protected@file@percent }
\newlabel{dfn:conv_prob}{{1.6.1}{22}{}{dfn.1.6.1}{}}
\newlabel{dfn:conv_cs}{{1.6.2}{22}{}{dfn.1.6.2}{}}
\newlabel{dfn:conv_debil}{{1.6.3}{22}{}{dfn.1.6.3}{}}
\newlabel{dfn:conv_ley}{{1.6.4}{22}{}{dfn.1.6.4}{}}
\newlabel{thm:rel_convergencias}{{1.6.1}{22}{}{thm.1.6.1}{}}
\newlabel{thm:conv_law_to_prob}{{1.6.2}{22}{}{thm.1.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Desigualdades}{22}{section.1.7}\protected@file@percent }
\newlabel{prop:markov}{{1.7.2}{23}{}{prop.1.7.2}{}}
\newlabel{prop:chebyshev}{{1.7.3}{23}{}{prop.1.7.3}{}}
\newlabel{prop:variance_zero}{{1.7.4}{23}{}{prop.1.7.4}{}}
\newlabel{prop:jensen}{{1.7.5}{23}{}{prop.1.7.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Momentos de un vector aleatorio}{23}{section.1.8}\protected@file@percent }
\newlabel{dfn:expected_value_vector}{{1.8.1}{23}{}{dfn.1.8.1}{}}
\newlabel{dfn:expected_value_vector_cont}{{1.8.2}{23}{}{dfn.1.8.2}{}}
\newlabel{dfn:momento_conjunto}{{1.8.3}{23}{}{dfn.1.8.3}{}}
\newlabel{dfn:momento_conjunto_central}{{1.8.4}{23}{}{dfn.1.8.4}{}}
\newlabel{dfn:covarianza}{{1.8.5}{23}{}{dfn.1.8.5}{}}
\newlabel{dfn:matriz_covarianzas}{{1.8.6}{23}{}{dfn.1.8.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Resultados de la Estadística Inferencial}{24}{section.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.1}Las Leyes de los Grandes Números}{24}{subsection.1.9.1}\protected@file@percent }
\newlabel{thm:ley_debil}{{1.9.1}{25}{}{thm.1.9.1}{}}
\newlabel{thm:ley_fuerte}{{1.9.2}{25}{}{thm.1.9.2}{}}
\newlabel{cor:divergence_positive_infinite_mean}{{1.9.2.1}{25}{}{cor.1.9.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.2}La distribución empírica}{26}{subsection.1.9.2}\protected@file@percent }
\newlabel{thm:empirical_fn_convergence}{{1.9.3}{26}{}{thm.1.9.3}{}}
\newlabel{thm:glivenko_cantelli}{{1.9.4}{26}{}{thm.1.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.3}El Teorema Central del Límite}{27}{subsection.1.9.3}\protected@file@percent }
\newlabel{thm:de_moivre_laplace}{{1.9.5}{27}{}{thm.1.9.5}{}}
\newlabel{thm:lindeberg}{{1.9.6}{27}{}{thm.1.9.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Un acercamiento amistoso a la Teoría del Aprendizaje Estadístico}{29}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introducción}{29}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}La historia del aprendizaje automático}{30}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Los orígenes de la Teoría del Aprendizaje}{30}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}La década de 1960: Fundamentos de la Teoría del Aprendizaje Estadístico}{32}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}La década de 1970: La Minimización del Riesgo Estructural}{32}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}La década de 1980: Primeras aplicaciones prácticas y consolidación teórica}{33}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}La década de 1990: El surgimiento de las Máquinas de Vectores de Soporte}{33}{subsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}La década de 2000: Expansión de métodos y auge del ensemble learning}{33}{subsection.2.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}La actualidad: Deep Learning y nuevos paradigmas}{34}{subsection.2.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Arquitecturas modernas en NLP: Transformers, BERT y GPT}{34}{subsection.2.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Conceptos básicos del aprendizaje}{35}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Pérdida y Riesgo}{37}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Espacios de funciones y el clasificador de Bayes}{38}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Generalización}{39}{subsection.2.3.3}\protected@file@percent }
\newlabel{dfn:riesgo_empirico}{{2.3.2}{39}{}{dfn.2.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Algortimos de aprendizaje}{39}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Consistencia}{40}{subsection.2.3.5}\protected@file@percent }
\newlabel{def:consistencias}{{2.3.4}{41}{}{Item.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Dilemas del aprendizaje}{42}{subsection.2.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Dos posibles modelos de regresión, uno lineal y otro no, para un conjunto de datos dado. (Gráfica tomada de [1])}}{43}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Sobreajuste y subajuste}{{2.1}{43}{Dos posibles modelos de regresión, uno lineal y otro no, para un conjunto de datos dado. (Gráfica tomada de [1])}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Error de estimación y de aproximación. (Gráfica tomada de [1])}}{46}{figure.caption.5}\protected@file@percent }
\newlabel{fig:error de estimación y aproximación}{{2.2}{46}{Error de estimación y de aproximación. (Gráfica tomada de [1])}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Compromiso entre error de estimación y de aproximación. Si el espacio funcional $\mathcal  {F}$ que define el algoritmo de aprendizaje es pequeño, es decir es poco complejo, el error de estimación es bajo, pero el de aproximación es alto, y estamos en una situación proclive al subajuste. Si, por otro lado, $\mathcal  {F}$ es complejo, el error de estimación es alto y el de aproximación bajo, y tendemos al sobreajuste. El error ideal es usualmente obtenido con una complejidad moderada. (Gráfica tomada de [1])}}{47}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Compromiso entre error de estimación y de aproximación.}{{2.3}{47}{Compromiso entre error de estimación y de aproximación. Si el espacio funcional $\mathcal {F}$ que define el algoritmo de aprendizaje es pequeño, es decir es poco complejo, el error de estimación es bajo, pero el de aproximación es alto, y estamos en una situación proclive al subajuste. Si, por otro lado, $\mathcal {F}$ es complejo, el error de estimación es alto y el de aproximación bajo, y tendemos al sobreajuste. El error ideal es usualmente obtenido con una complejidad moderada. (Gráfica tomada de [1])}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}El clasificador de los $k$ vecinos más cercanos}{47}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Minimización del riesgo empírico}{51}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Aplicando la ley de los grandes números}{52}{subsection.2.5.1}\protected@file@percent }
\newlabel{eq:Chernoff-Hoeffding}{{2.1}{53}{Aplicando la ley de los grandes números}{equation.2.1}{}}
\newlabel{eq:Chernoff}{{2.2}{53}{Aplicando la ley de los grandes números}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Inconsistencia en la minimización del riesgo empírico}{54}{subsection.2.5.2}\protected@file@percent }
\newlabel{Inconsistencia en la minimización del riesgo empírico}{{2.5.2}{54}{Inconsistencia en la minimización del riesgo empírico}{subsection.2.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Convergencia uniforme}{55}{subsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Representación simplificada de la convergencia del riesgo empírico al riesgo verdadero. El eje \(x\) representa una dimensión de la clase de funciones \(F\), mientras que el eje \(y\) denota el riesgo. Para cada función fija \(f\), la ley de los grandes números nos dice que, a medida que el tamaño de la muestra tiende a infinito, el riesgo empírico \(\mathcal  {R}_{\text  {emp}}(f)\) converge al riesgo verdadero \(\mathcal  {R}(f)\) (indicado por la flecha). Sin embargo, esto no implica que, en el límite de tamaños de muestra infinitos, el minimizador del riesgo empírico, \(f_n\), lleve a un valor del riesgo tan bueno como el del mejor \(f_F\) en la clase de funciones. Para que esto sea cierto, requerimos la convergencia uniforme de \(\mathcal  {R}_{\text  {emp}}(f)\) a \(\mathcal  {R}(f)\) sobre todas las funciones en \(F\). (Grafico adaptado de [5].}}{56}{figure.caption.7}\protected@file@percent }
\newlabel{riesgo vs riesgo empirico}{{2.4}{56}{Representación simplificada de la convergencia del riesgo empírico al riesgo verdadero. El eje \(x\) representa una dimensión de la clase de funciones \(F\), mientras que el eje \(y\) denota el riesgo. Para cada función fija \(f\), la ley de los grandes números nos dice que, a medida que el tamaño de la muestra tiende a infinito, el riesgo empírico \(\mathcal {R}_{\text {emp}}(f)\) converge al riesgo verdadero \(\mathcal {R}(f)\) (indicado por la flecha). Sin embargo, esto no implica que, en el límite de tamaños de muestra infinitos, el minimizador del riesgo empírico, \(f_n\), lleve a un valor del riesgo tan bueno como el del mejor \(f_F\) en la clase de funciones. Para que esto sea cierto, requerimos la convergencia uniforme de \(\mathcal {R}_{\text {emp}}(f)\) a \(\mathcal {R}(f)\) sobre todas las funciones en \(F\). (Grafico adaptado de [5]}{figure.caption.7}{}}
\newlabel{eq:cota_uniforme}{{2.3}{56}{Convergencia uniforme}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Cotas de generalización y medidas de capacidad}{58}{section.2.6}\protected@file@percent }
\newlabel{eq:med_unif_riesgo_riesgo_emp}{{2.4}{58}{Cotas de generalización y medidas de capacidad}{equation.2.4}{}}
\newlabel{eq: chernoff acotada por m}{{2.5}{59}{Cotas de generalización y medidas de capacidad}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Simetrización}{59}{subsection.2.6.1}\protected@file@percent }
\newlabel{lema de simetrizacion}{{2.6.1}{59}{de simetrización}{lem.2.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}El coeficiente de fragmentación}{60}{subsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Dos puntos en $\mathbb  {R}^2$ son clasificados de igual manera por dos clasificadores distintos.}}{61}{figure.caption.8}\protected@file@percent }
\newlabel{clasificacion identica con distintos clasificadores}{{2.5}{61}{Dos puntos en $\mathbb {R}^2$ son clasificados de igual manera por dos clasificadores distintos}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Fragmentación de muestras en $\mathbb  {R}^2$ de tres y cuatro puntos. (Gráfica tomada de [11])}}{62}{figure.caption.9}\protected@file@percent }
\newlabel{fragmentacion perceptron}{{2.6}{62}{Fragmentación de muestras en $\mathbb {R}^2$ de tres y cuatro puntos. (Gráfica tomada de [11])}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Cotas de convergencia uniforme}{63}{subsection.2.6.3}\protected@file@percent }
\newlabel{eq: cota convergencia coeficiente fragmentacion}{{2.6}{64}{Cotas de convergencia uniforme}{equation.2.6}{}}
\newlabel{thm: consistencia ssi}{{2.6.1}{64}{}{thm.2.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Cotas de generalización}{65}{subsection.2.6.4}\protected@file@percent }
\newlabel{eq: cota generalizacion}{{2.7}{65}{Cotas de generalización}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}La dimensión VC}{66}{subsection.2.6.5}\protected@file@percent }
\newlabel{def:cota combinatoria de dimensión VC}{{2.6.2}{67}{}{lem.2.6.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}La Teoría VC}{69}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Definiciones preliminares}{69}{section.3.1}\protected@file@percent }
\newlabel{def: probabilidad en espacio muestral}{{3.1}{70}{}{equation.3.1}{}}
\newlabel{def: funcion pi}{{3.2}{71}{Definiciones preliminares}{equation.3.2}{}}
\newlabel{def: combinatoria función crecimiento lineal}{{3.3}{73}{}{equation.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}La función de crecimiento}{74}{section.3.2}\protected@file@percent }
\newlabel{Lema 1}{{3.2.1}{74}{}{lem.3.2.1}{}}
\newlabel{teo: cota polinomica de la función de crecimiento}{{3.2.1}{74}{}{thm.3.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}El Lema de simetrización}{76}{section.3.3}\protected@file@percent }
\newlabel{def Z2l prod interno}{{3.4}{77}{El Lema de simetrización}{equation.3.4}{}}
\newlabel{desigualdad P^(2l)(C)}{{3.6}{78}{El Lema de simetrización}{equation.3.6}{}}
\newlabel{eq: theta como probabilidad}{{3.7}{78}{El Lema de simetrización}{equation.3.7}{}}
\newlabel{Chebyshev aplicada a frecuencia relativa}{{3.8}{79}{El Lema de simetrización}{equation.3.8}{}}
\newlabel{eq:probabilidad integral con permutaciones}{{3.9}{82}{El Lema de simetrización}{equation.3.9}{}}
\newlabel{promedio de tita acotado por la funcion de crecimiento}{{3.10}{83}{El Lema de simetrización}{equation.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Condiciones de convergencia uniforme casi segura}{84}{section.3.4}\protected@file@percent }
\newlabel{Aplicacion del lema de Borel-Cantelli}{{3.4.1}{84}{}{lem.3.4.1}{}}
\newlabel{obs:desigualdad triangular indice de un sistema}{{3.4.1}{86}{}{obs.3.4.1}{}}
\newlabel{subaditividad de la entropia 1}{{3.4.2}{86}{}{obs.3.4.2}{}}
\newlabel{subaditivdad de la entropia 2}{{3.4.3}{87}{}{obs.3.4.3}{}}
\newlabel{lema 3}{{3.4.2}{87}{}{lem.3.4.2}{}}
\newlabel{cota de la entropia}{{3.11}{87}{Condiciones de convergencia uniforme casi segura}{equation.3.11}{}}
\newlabel{cota inferior mas epsilon de la entropia}{{3.12}{87}{Condiciones de convergencia uniforme casi segura}{equation.3.12}{}}
\newlabel{corolario de la cota del límite de la entropia}{{3.4.1.1}{88}{}{cor.3.4.1.1}{}}
\newlabel{lema 4}{{3.4.3}{89}{}{lem.3.4.3}{}}
\newlabel{desigualdad 14}{{3.13}{89}{Condiciones de convergencia uniforme casi segura}{equation.3.13}{}}
\newlabel{desigualdad 15}{{3.14}{90}{Condiciones de convergencia uniforme casi segura}{equation.3.14}{}}
\newlabel{limite de P+}{{3.15}{91}{Condiciones de convergencia uniforme casi segura}{equation.3.15}{}}
\newlabel{eq 16}{{3.16}{91}{Condiciones de convergencia uniforme casi segura}{equation.3.16}{}}
\newlabel{eq 17}{{3.17}{91}{Condiciones de convergencia uniforme casi segura}{equation.3.17}{}}
\newlabel{eq 18}{{3.18}{92}{Condiciones de convergencia uniforme casi segura}{equation.3.18}{}}
\newlabel{eq 19}{{3.19}{93}{Condiciones de convergencia uniforme casi segura}{equation.3.19}{}}
\newlabel{eq 20}{{3.21}{93}{Condiciones de convergencia uniforme casi segura}{equation.3.21}{}}
\newlabel{condicion nec y suf}{{3.22}{93}{}{equation.3.22}{}}
\newlabel{eq 23}{{3.24}{95}{Condiciones de convergencia uniforme casi segura}{equation.3.24}{}}
\newlabel{eq 24}{{3.25}{96}{Condiciones de convergencia uniforme casi segura}{equation.3.25}{}}
\newlabel{eq 25}{{3.26}{97}{Condiciones de convergencia uniforme casi segura}{equation.3.26}{}}
\newlabel{eq 26}{{3.27}{97}{Condiciones de convergencia uniforme casi segura}{equation.3.27}{}}
\newlabel{eq 27}{{3.28}{99}{Condiciones de convergencia uniforme casi segura}{equation.3.28}{}}
\newlabel{eq 28}{{3.29}{100}{Condiciones de convergencia uniforme casi segura}{equation.3.29}{}}
\newlabel{eq 29}{{3.30}{100}{Condiciones de convergencia uniforme casi segura}{equation.3.30}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Otros conceptos del Aprendizaje}{101}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Más medidas de capacidad de un espacio de clasificadores}{101}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Complejidad de Rademacher}{101}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Cotas con grandes márgenes de separación}{102}{subsection.4.1.2}\protected@file@percent }
\newlabel{def:margen}{{4.1.2}{103}{}{dfn.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Margen de un clasificador lineal: las cruces representan puntos de entrenamiento con etiqueta \(+1\), mientras que los círculos representan puntos de entrenamiento con etiqueta \(-1\). La línea recta es el clasificador lineal \(f_n\), y la línea discontinua muestra el margen. La anchura \(\rho \) del margen está representada por la flecha.(Gráfica tomada de [1])}}{103}{figure.caption.10}\protected@file@percent }
\newlabel{fig:Clasificación con margen}{{4.1}{103}{Margen de un clasificador lineal: las cruces representan puntos de entrenamiento con etiqueta \(+1\), mientras que los círculos representan puntos de entrenamiento con etiqueta \(-1\). La línea recta es el clasificador lineal \(f_n\), y la línea discontinua muestra el margen. La anchura \(\rho \) del margen está representada por la flecha.(Gráfica tomada de [1])}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Conclusiones acerca de las cotas de generalización}{104}{subsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Consistencia de Bayes y error de aproximación}{104}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Espacios funcionales anidados}{105}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Regularización}{107}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Los teoremas de la chancha y los veinte}{109}{section.4.3}\protected@file@percent }
\newlabel{eq:cota_parametro_b_distribuciones}{{4.1}{112}{Los teoremas de la chancha y los veinte}{equation.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Tomando en consideración todas las distribuciones posibles -eje X-, el clasificador 1 es en general mejor que el clasificador 2, y este último es muchísimo mejor para un subconjunto de distribuciones. Podemos pensar que 1 es un clasificador general y 2 está construído para un tipo de problemas específico. El teorema de la chancha y los veinte nos dice que el área debajo de ambas curvas debe ser igual, es decir que en promedio ambos clasificadores tienen el mismo error. (Gráfica tomada de [1])}}{114}{figure.caption.11}\protected@file@percent }
\newlabel{fig:no_free_lunch_theorem}{{4.2}{114}{Tomando en consideración todas las distribuciones posibles -eje X-, el clasificador 1 es en general mejor que el clasificador 2, y este último es muchísimo mejor para un subconjunto de distribuciones. Podemos pensar que 1 es un clasificador general y 2 está construído para un tipo de problemas específico. El teorema de la chancha y los veinte nos dice que el área debajo de ambas curvas debe ser igual, es decir que en promedio ambos clasificadores tienen el mismo error. (Gráfica tomada de [1])}{figure.caption.11}{}}
\gdef \@abspage@last{117}
